{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, BatchNormalization, Dropout\n",
    "import random\n",
    "\n",
    "img_width = 128\n",
    "img_height = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number(num):\n",
    "    if num < 10:\n",
    "        return \"0\" + str(num)\n",
    "    return str(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_frames(video_path, output_dir, num_frames=60):\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))  # get the total number of frames\n",
    "    start_frame = max(0, total_frames - num_frames)  # calculate the starting frame for the last n frames\n",
    "\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    while success:\n",
    "        if count >= start_frame:  # only save frames from the last n frames\n",
    "            height, width, _ = image.shape  # get the dimensions of the frame\n",
    "            right_half = image[:, width//2:]  # select the right half of the frame\n",
    "            cv2.imwrite(output_dir + \"/frame\" + get_number(count - start_frame) + \".jpg\", right_half)  # save frame as JPEG file\n",
    "        success, image = vidcap.read()\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"C:/Users/M/OneDrive - softromic/Documents/TreeHacks Fall Detection/falls\"\n",
    "dest = \"C:/Users/M/OneDrive - softromic/Documents/TreeHacks Fall Detection/video frames/vidframes - \"\n",
    "for i in range(30):\n",
    "    video_to_frames(directory + \"/fall-\" + str(i+1) + \"-cam0.mp4\", dest + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"C:/Users/M/OneDrive - softromic/Documents/TreeHacks Fall Detection/nonfalls\"\n",
    "dest = \"C:/Users/M/OneDrive - softromic/Documents/TreeHacks Fall Detection/nonfall video frames/vidframes - \"\n",
    "for i in range(40):\n",
    "    video_to_frames(directory + \"/adl-\" + str(i+1) + \"-cam0.mp4\", dest + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames_from_folder(folder_path):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(folder_path)):  # assuming all frames are named in ascending order\n",
    "        img = cv2.imread(os.path.join(folder_path, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "            img = img / 255\n",
    "            images.append(img)\n",
    "    images = np.array(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inv_frames_from_folder(folder_path):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(folder_path)):  # assuming all frames are named in ascending order\n",
    "        img = cv2.imread(os.path.join(folder_path, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "            img = img / 255\n",
    "            img = cv2.flip(img, 1)\n",
    "            images.append(img)\n",
    "    images = np.array(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dark_frames_from_folder(folder_path):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(folder_path)):  # assuming all frames are named in ascending order\n",
    "        img = cv2.imread(os.path.join(folder_path, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "            img = img / 255\n",
    "            img = img / random.uniform(1.3, 1.7)\n",
    "            images.append(img)\n",
    "    images = np.array(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dark_inv_frames_from_folder(folder_path):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(folder_path)):  # assuming all frames are named in ascending order\n",
    "        img = cv2.imread(os.path.join(folder_path, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "            img = img / 255\n",
    "            img = cv2.flip(img, 1)\n",
    "            img = img / random.uniform(1.3, 1.7)\n",
    "            images.append(img)\n",
    "    images = np.array(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_frames(dir, num_frames, style):\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for video_folder in os.listdir(dir):\n",
    "        if style == 'reg':\n",
    "            video_frames = load_frames_from_folder(os.path.join(dir, video_folder))\n",
    "        elif style == 'inv':\n",
    "            video_frames = load_inv_frames_from_folder(os.path.join(dir, video_folder))\n",
    "        elif style == 'dark':\n",
    "            video_frames = load_dark_frames_from_folder(os.path.join(dir, video_folder))\n",
    "        elif style == 'dark_inv':\n",
    "            video_frames = load_dark_inv_frames_from_folder(os.path.join(dir, video_folder))\n",
    "\n",
    "        video_frames = np.array(video_frames)\n",
    "        \n",
    "        # Ensure all videos have the same number of frames\n",
    "        if len(video_frames) < num_frames:\n",
    "            # If the video has fewer than num_frames frames, pad it with zeros\n",
    "            padding = np.zeros((num_frames - len(video_frames),) + video_frames.shape[1:])\n",
    "            video_frames = np.concatenate([video_frames, padding])\n",
    "        elif len(video_frames) > num_frames:\n",
    "            # If the video has more than num_frames frames, truncate it\n",
    "            video_frames = video_frames[:num_frames]\n",
    "        frames.append(video_frames)\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(dir, num_frames):\n",
    "    reg_frames = np.array(compile_frames(dir, num_frames, \"reg\"))\n",
    "    inv_frames = np.array(compile_frames(dir, num_frames, \"inv\"))\n",
    "    dark_frames = np.array(compile_frames(dir, num_frames, \"dark\"))\n",
    "    dark_inv_frames = np.array(compile_frames(dir, num_frames, \"dark_inv\"))\n",
    "\n",
    "    frames = np.concatenate((reg_frames, inv_frames), axis = 0)\n",
    "    frames = np.concatenate((frames, dark_frames), axis = 0)\n",
    "    frames = np.concatenate((frames, dark_inv_frames), axis = 0)\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 506. MiB for an array with shape (30, 60, 96, 128, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m num_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[0;32m      3\u001b[0m fall_frame_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/M/OneDrive - softromic/Documents/TreeHacks Fall Detection/video frames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m fall_videos_frames \u001b[38;5;241m=\u001b[39m \u001b[43mget_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfall_frame_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m nonfall_frame_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/M/OneDrive - softromic/Documents/TreeHacks Fall Detection/nonfall video frames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m nonfall_videos_frames \u001b[38;5;241m=\u001b[39m get_frames(nonfall_frame_dir, num_frames)\n",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m, in \u001b[0;36mget_frames\u001b[1;34m(dir, num_frames)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_frames\u001b[39m(\u001b[38;5;28mdir\u001b[39m, num_frames):\n\u001b[0;32m      2\u001b[0m     reg_frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(compile_frames(\u001b[38;5;28mdir\u001b[39m, num_frames, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m     inv_frames \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompile_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     dark_frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(compile_frames(\u001b[38;5;28mdir\u001b[39m, num_frames, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdark\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m     dark_inv_frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(compile_frames(\u001b[38;5;28mdir\u001b[39m, num_frames, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdark_inv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 506. MiB for an array with shape (30, 60, 96, 128, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "num_frames = 60\n",
    "\n",
    "fall_frame_dir = \"C:/Users/M/OneDrive - softromic/Documents/TreeHacks Fall Detection/video frames\"\n",
    "fall_videos_frames = get_frames(fall_frame_dir, num_frames)\n",
    "\n",
    "nonfall_frame_dir = \"C:/Users/M/OneDrive - softromic/Documents/TreeHacks Fall Detection/nonfall video frames\"\n",
    "nonfall_videos_frames = get_frames(nonfall_frame_dir, num_frames)\n",
    "\n",
    "all_videos_frames = np.concatenate((fall_videos_frames, nonfall_videos_frames), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m all_videos_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mfall_videos_frames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m      3\u001b[0m     all_videos_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nonfall_videos_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "all_videos_labels = []\n",
    "for i in range(fall_videos_frames.shape[0]):\n",
    "    all_videos_labels.append(1)\n",
    "for i in range(nonfall_videos_frames.shape[0]):\n",
    "    all_videos_labels.append(0)\n",
    "\n",
    "all_videos_labels =  np.array(all_videos_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\M\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\M\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\M\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDist  (None, 60, 96, 128, 32)   896       \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 60, 48, 64, 32)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 60, 48, 64, 64)    18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDi  (None, 60, 24, 32, 64)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDi  (None, 60, 24, 32, 128)   73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDi  (None, 60, 12, 16, 128)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, 60, 24576)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                6308096   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6405569 (24.44 MB)\n",
      "Trainable params: 6405569 (24.44 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "frame_shape = (img_height, img_width, 3)  # Height, Width, Channels\n",
    "\n",
    "# Define the number of frames in each sequence.\n",
    "sequence_length = 60\n",
    "\n",
    "model = Sequential([\n",
    "    # TimeDistributed wrapper allows applying the same layers individually to each time step.\n",
    "    TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=(sequence_length,) + frame_shape),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "    TimeDistributed(Dropout(0.25)),\n",
    "    \n",
    "    TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "    TimeDistributed(Dropout(0.25)),\n",
    "    \n",
    "    TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "    TimeDistributed(Dropout(0.25)),\n",
    "    \n",
    "    TimeDistributed(Flatten()),\n",
    "    \n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\M\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\M\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 102s 42s/step - loss: 0.9720 - accuracy: 0.5179 - val_loss: 0.7044 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 76s 37s/step - loss: 0.7060 - accuracy: 0.6071 - val_loss: 0.7269 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 72s 29s/step - loss: 0.6882 - accuracy: 0.5893 - val_loss: 0.7382 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 73s 36s/step - loss: 0.6503 - accuracy: 0.5893 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 60s 28s/step - loss: 0.6094 - accuracy: 0.5893 - val_loss: 0.6479 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 80s 35s/step - loss: 0.6687 - accuracy: 0.5893 - val_loss: 0.7108 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 84s 34s/step - loss: 0.5831 - accuracy: 0.5893 - val_loss: 0.5806 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 68s 35s/step - loss: 0.4854 - accuracy: 0.6071 - val_loss: 0.4121 - val_accuracy: 0.9286\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 202s 34s/step - loss: 0.3462 - accuracy: 0.9464 - val_loss: 0.2720 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 74s 34s/step - loss: 0.2573 - accuracy: 1.0000 - val_loss: 0.1715 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b632db3350>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(all_videos_frames.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "all_videos_frames = all_videos_frames[indices]\n",
    "all_videos_labels = all_videos_labels[indices]\n",
    "\n",
    "model.fit(all_videos_frames, all_videos_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:/Users/M/OneDrive - softromic/Documents/TreeHacks Fall Detection/models/model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
